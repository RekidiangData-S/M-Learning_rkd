{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **simple linear regression**   : When there is a single input variable (x)\n",
    "> * **multiple linear regression** : When there is multiple input variable (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data For Linear Regression\n",
    "\n",
    "#### Linear Regression Preparing Data Heuristic :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **Linear Assumption**. Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n",
    "\n",
    "> * **Remove Noise**. Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n",
    "\n",
    "> * **Remove Collinearity**. Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n",
    "\n",
    "> * **Gaussian Distributions**. Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n",
    "\n",
    "> * **Rescale Inputs**: Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Learning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Simple Linear Regression Model\n",
    "$$y = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "Where :\n",
    "\n",
    "> * y : the target/dependent variable\n",
    "> * x : the pedictor/independent variable\n",
    "> * $\\theta_0$ : the bias\n",
    "> * $\\theta_1$ : the weight in the regression equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Multiple Linear Regression Model\n",
    "$$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + ... + \\theta_n x_n$$\n",
    "\n",
    "Where :\n",
    "\n",
    "> * y : the target/dependent variable\n",
    "> * $x_1, x_2, x_3 ... x_n$  : the pedictor/independent variable\n",
    "> * $\\theta_0$ : the bias\n",
    "> * $\\theta_1, \\theta_2, \\theta_3, ... \\theta_n$ : the weight in the regression equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Polinomial Linear Regression Model\n",
    "$$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 {x_2}^2 + \\theta_3 {x_3}^3 + ... + \\theta_n {x_n}^n$$\n",
    "\n",
    "Where :\n",
    "\n",
    "> * y : the target/dependent variable\n",
    "> * $x_1, x_2, x_3 ... x_n$  : the pedictor/independent variable\n",
    "> * $\\theta_0$ : the bias\n",
    "> * $\\theta_1, \\theta_2, \\theta_3, ... \\theta_n$ : the weight in the regression equation\n",
    "> * n :  the degree of the polynomial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\theta_0 + \\theta_1 x$$\n",
    "$$\\hat{\\theta_0} = min_{\\theta_0}\\sum_{i=1}^{n} {(y_i - \\theta_0 - \\theta_1 x)}^2 = min_{\\theta_0}\\sum_{i=1}^{n} {\\epsilon_i}^2$$ \n",
    "$$\\hat{\\theta_1} = min_{\\theta_1}\\sum_{i=1}^{n} {(y_i - \\theta_0 - \\theta_1 x)}^2 = min_{\\theta_1}\\sum_{i=1}^{n} {\\epsilon_i}^2$$ \n",
    "\n",
    "> The above  procedure is called Ordinary Least Squared error — OLS.\n",
    "\n",
    "#### Let’s demonstrate those optimization problems step by step. If we reframe our squared error sum as follows:\n",
    "\n",
    "$$\\sum_{i=1}^{n} {(y_i - \\theta_0 - \\theta_1 *  x)}^2 = S(\\hat{\\theta_0}, \\hat{\\theta_1})$$ \n",
    "\n",
    "> Assume as follow :\n",
    "$$\\hat{\\theta_0} : \\frac{\\delta S(\\hat{\\theta_0}, \\hat{\\theta_1)}}{\\delta \\hat{\\theta_0}} = 0$$ \n",
    "$$\\hat{\\theta_1} : \\frac{\\delta S(\\hat{\\theta_0}, \\hat{\\theta_1)}}{\\delta \\hat{\\theta_1}} = 0$$ \n",
    "\n",
    "> So let’s start with $\\theta_1$:\n",
    "\n",
    "$$\\sum_{i=1}^{n} {(y_i - \\theta_0 - \\theta_1 *  x_i)}x_1 = 0$$\n",
    "\n",
    " $$=> \\sum_{i=1}^{n} y_i x_i - \\theta_0 x_i - \\theta_1 {x_i}^2 = 0 $$\n",
    " $$=> \\sum_{i=1}^{n} y_ix_i - (\\bar{y_i} - \\hat{\\theta_1} \\bar{x}) x_i - \\hat{\\theta_1} {x_i}^2 = 0 $$\n",
    " $$=> \\sum_{i=1}^{n} y_ix_i - \\bar{y_i} x_i - \\hat{\\theta_1} \\bar{x}x - \\hat{\\theta_1} {x_i}^2 = 0 = => \\sum_{i=1}^{n} (y_i - \\bar{y} + \\hat{\\theta_1} \\bar{x} - \\hat{\\theta_1} {x_i})x_i = 0$$\n",
    "$$\\sum_{i=1}^{n} (y_i - \\bar{y} + \\hat{\\theta_1}( \\bar{x} - x_i) = 0 => \\sum_{i=1}^{n} (y_i - \\bar{y}) = - \\hat{\\theta_1}\\sum_{i=1}^{n} (x_i - \\bar{x})$$\n",
    "\n",
    "$$\\hat{\\theta_1} = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})}$$\n",
    "$$\\hat{\\theta_1} = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^{n} {(x_i - \\bar{x})}^2}$$\n",
    "\n",
    "Knowing that the sample covariance between two variables is given by:\n",
    "\n",
    "$$\\sigma_xy = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$$\n",
    "\n",
    "And knowing that the sample correlation coefficient between two variables is equal to:\n",
    "\n",
    "$$\\rho = \\frac{\\sigma_xy}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "We can reframe the above expression as follows:\n",
    "\n",
    "$$\\hat{\\theta_1} = \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^{n} {(x_i - \\bar{x})}^2} = \\sigma_xy = \\frac{\\sigma_xy}{{\\sigma_x}^2} =  \\rho_xy\\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "> The same reasoning holds for our $\\theta_0$:\n",
    "\n",
    "$$\\sum_{i=1}^{n} {(y_i - \\hat{\\theta_0} - \\hat{\\theta_1} *  x_i)} = 0$$\n",
    "\n",
    "==> $$\\hat{\\theta_0} = \\sum_{i=1}^{n} (y_i  - \\hat{\\theta_1} *  x_i) ==> \\hat{\\theta_0} =  \\bar{y}  - \\bar{\\theta_1} *  \\bar{x} $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once obtained those values of α and β which minimize the squared errors, our model’s equation will look like that:\n",
    "$$y = \\hat{\\theta_0} + \\hat{\\theta_0}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **regularization methods**. These seek to both minimize the sum of the squared error of the model on the training data (using **ordinary least squares** or **Gradient Descent**) but also to reduce the complexity of the model (like the number or absolute size of the sum of all coefficients in the model).\n",
    "\n",
    "> Two popular examples of regularization procedures for linear regression are:\n",
    "> * **Lasso Regression:** where Ordinary Least Squares is modified to also minimize the absolute sum of the coefficients (called L1 regularization).\n",
    "> * **Ridge Regression:** where Ordinary Least Squares is modified to also minimize the squared absolute sum of the coefficients (called L2 regularization).\n",
    "\n",
    "#### **N.B :** These methods are effective to use when there is collinearity in your input values and ordinary least squares would overfit the training data.\n",
    "\n",
    "\n",
    "++++>>> Hyperparameters and Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Imagine we are predicting weight (y) from height (x). Our linear regression model representation for this problem would be:\n",
    "$$y = \\theta_0 + \\theta_1 x$$\n",
    "$$weight = \\theta_0 + \\theta_1 height$$\n",
    "\n",
    "> If After training the model We  find a good set of coefficient values, for example \n",
    "$\\theta_0 = 0.1$   and   $\\theta_1 = 0.5$\n",
    "\n",
    "> Knowing parameters $\\theta_0$   and   $\\theta_1$ \n",
    "\n",
    "> values and the input(height) value, Now let predict or calculate the weight (in kilograms) for a person with the height of 182 centimeters.\n",
    "$$weight = 0.1 + 0.5 * 182$$\n",
    "$$weight = 91.1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mean Squared Error (MSE) called the coefficient of determination is Linear Regression model's evaluation metric\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})$$\n",
    "\n",
    "> This is a value between 0 and 1 for no-fit and perfect fit respectively. It shows which is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "If we consider the following values:\n",
    "\n",
    "> * Total Sum of Squares (TSS) $TSS = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\bar{y_i})$\n",
    "> * Explained Sum of Squares (ESS) $TSS = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y_i})$\n",
    "> * Explained Sum of Squares (ESS) $TSS = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})$\n",
    "\n",
    "and knowing that the following relation is true:\n",
    "$$TSS = RSS + ESS$$\n",
    "We can compute our R Squared as follows:\n",
    "$$R^2 = \\frac{RSS}{TSS} = 1 - \\frac{ESS}{TSS}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources :**\n",
    "> * https://machinelearningmastery.com/linear-regression-for-machine-learning/\n",
    "> * https://www.analyticsvidhya.com/blog/2020/03/polynomial-regression-python/\n",
    "> * https://towardsdatascience.com/understanding-the-ols-method-for-simple-linear-regression-e0a4e8f692cc\n",
    "> * https://medium.com/datadriveninvestor/simple-linear-regression-with-python-1b028386e5cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
